{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f274c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import numpy as np\n",
    "import functools\n",
    "import json\n",
    "import math\n",
    "import os \n",
    "\n",
    "%run midi_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a4738",
   "metadata": {},
   "source": [
    "## Additional preprocessing\n",
    "\n",
    "In this .ipynb, some additional fixes is done to better the encoding made in midi_utils.\n",
    "For an example, since Music21 often deals with fractions, the rounding and subtraction done in midi_utils resulted in some problems, such as values that should've resulted in 0 resulting in 0.001 or 0.333 resulting in 0.334. These are fixed here.\n",
    "\n",
    "In addition, musical piece starting and ending tokens are added and rare (seen less than 100 times in the entire dataset) wait values are replaced with their closest often-occurring counterpart.\n",
    "\n",
    "Finally, the resulting data is turned into integers, with the token-integer pairing saved to vocabulary.json and a .txt file is generated, where each line represents a 100 integers long sequence input and a single integer output, which will be training data for the RNN. \n",
    "\n",
    "<strong>Note</strong> that due the generated training data file has 2M lines (~600MB), so to use it for your own training you have to run this entire notebook, because GitHub wouldn't be happy if I uploaded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "76ce8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all midi tokens into a single list of lists\n",
    "filename = './training_data.txt'\n",
    "with open(filename) as file:\n",
    "    training_data = file.read().splitlines() \n",
    "    \n",
    "# Split each midi by space to get its original token list form\n",
    "training_data = [tokens.split(\" \") for tokens in training_data]\n",
    "\n",
    "# Add beginning and ending tokens\n",
    "for tokens in training_data:\n",
    "    tokens.insert(0, \"<BOS>\")\n",
    "    tokens.append(\"<EOS>\")\n",
    "    \n",
    "# Handle rare wait values by finding rarely occurring wait values and replacing them with the closest often-occurring value\n",
    "training_data = [item for sublist in training_data for item in sublist]\n",
    "\n",
    "for i in range(len(training_data)):\n",
    "    if training_data[i].startswith(\"wait\"):\n",
    "        fraction_to_float = round(float(Fraction(training_data[i].split(\":\")[1])), 3)\n",
    "        training_data[i] = \"wait:\" + str(fraction_to_float)\n",
    "\n",
    "token_occurrences = Counter(training_data)\n",
    "occurrence_threshold = 100\n",
    "need_to_be_replaced = sorted(list({float(k.split(\":\")[1]) for k, v in sorted(token_occurrences.items(), key=lambda item: item[1]) if k.startswith(\"wait\") and v < occurrence_threshold}))\n",
    "occurring_often = sorted(list({float(k.split(\":\")[1]) for k, v in sorted(token_occurrences.items(), key=lambda item: item[1]) if k.startswith(\"wait\") and v >= occurrence_threshold}))\n",
    "\n",
    "# Find closest often-occurring value to each rarely occurring value\n",
    "replacements = list()\n",
    "for wait_time in need_to_be_replaced:\n",
    "    replacements.append([str(wait_time), str(min(occurring_often, key=lambda x:abs(x-wait_time)))])\n",
    "    \n",
    "for old, new in replacements:\n",
    "    old = \"wait:\" + str(old)\n",
    "    new = \"wait:\" + str(new)\n",
    "    training_data = [new if token == old else token for token in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2fd635eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 208\n",
      "Occurrences:\n",
      "note:106 \t 1\n",
      "stop_note:106 \t 1\n",
      "note:107 \t 1\n",
      "stop_note:107 \t 1\n",
      "note:21 \t 3\n",
      "stop_note:21 \t 3\n",
      "note:23 \t 4\n",
      "stop_note:23 \t 4\n",
      "note:105 \t 8\n",
      "stop_note:105 \t 8\n",
      "note:104 \t 16\n",
      "stop_note:104 \t 16\n",
      "note:22 \t 17\n",
      "stop_note:22 \t 17\n",
      "note:24 \t 65\n",
      "stop_note:24 \t 65\n",
      "note:103 \t 82\n",
      "stop_note:103 \t 82\n",
      "note:102 \t 101\n",
      "stop_note:102 \t 101\n",
      "wait:1.667 \t 114\n",
      "note:25 \t 117\n",
      "stop_note:25 \t 117\n",
      "note:26 \t 150\n",
      "stop_note:26 \t 150\n",
      "tempo:40 \t 239\n",
      "wait:4.0 \t 245\n",
      "wait:2.5 \t 248\n",
      "wait:1.75 \t 257\n",
      "note:27 \t 273\n",
      "stop_note:27 \t 273\n",
      "<BOS> \t 295\n",
      "<EOS> \t 295\n",
      "wait:1.25 \t 303\n",
      "note:28 \t 310\n",
      "stop_note:28 \t 310\n",
      "note:101 \t 341\n",
      "stop_note:101 \t 341\n",
      "wait:3.0 \t 387\n",
      "wait:0.667 \t 633\n",
      "note:100 \t 671\n",
      "stop_note:100 \t 671\n",
      "wait:0.417 \t 686\n",
      "note:98 \t 702\n",
      "stop_note:98 \t 702\n",
      "note:30 \t 729\n",
      "stop_note:30 \t 729\n",
      "note:99 \t 799\n",
      "stop_note:99 \t 799\n",
      "note:96 \t 1029\n",
      "stop_note:96 \t 1029\n",
      "note:29 \t 1049\n",
      "stop_note:29 \t 1049\n",
      "tempo:40.0 \t 1127\n",
      "note:97 \t 1140\n",
      "stop_note:97 \t 1140\n",
      "note:95 \t 1168\n",
      "stop_note:95 \t 1168\n",
      "wait:1.5 \t 1522\n",
      "note:31 \t 1551\n",
      "stop_note:31 \t 1551\n",
      "note:94 \t 1670\n",
      "stop_note:94 \t 1670\n",
      "wait:2.0 \t 1684\n",
      "note:32 \t 1740\n",
      "stop_note:32 \t 1740\n",
      "note:93 \t 1934\n",
      "stop_note:93 \t 1934\n",
      "note:33 \t 2061\n",
      "stop_note:33 \t 2061\n",
      "note:35 \t 2109\n",
      "stop_note:35 \t 2109\n",
      "note:92 \t 2352\n",
      "stop_note:92 \t 2352\n",
      "note:34 \t 2405\n",
      "stop_note:34 \t 2405\n",
      "tempo:50.0 \t 2549\n",
      "note:90 \t 2746\n",
      "stop_note:90 \t 2746\n",
      "note:91 \t 2829\n",
      "stop_note:91 \t 2829\n",
      "tempo:60.0 \t 3233\n",
      "note:37 \t 3283\n",
      "stop_note:37 \t 3283\n",
      "note:36 \t 3680\n",
      "stop_note:36 \t 3680\n",
      "tempo:70.0 \t 3690\n",
      "note:38 \t 3793\n",
      "stop_note:38 \t 3793\n",
      "note:42 \t 3988\n",
      "stop_note:42 \t 3988\n",
      "note:89 \t 4004\n",
      "stop_note:89 \t 4004\n",
      "note:39 \t 4099\n",
      "stop_note:39 \t 4099\n",
      "tempo:80.0 \t 4143\n",
      "note:40 \t 4238\n",
      "stop_note:40 \t 4238\n",
      "note:88 \t 4479\n",
      "stop_note:88 \t 4479\n",
      "note:87 \t 4616\n",
      "stop_note:87 \t 4616\n",
      "tempo:90.0 \t 5080\n",
      "tempo:140 \t 5260\n",
      "note:41 \t 5641\n",
      "stop_note:41 \t 5641\n",
      "note:85 \t 5689\n",
      "stop_note:85 \t 5689\n",
      "wait:0.75 \t 5775\n",
      "tempo:100.0 \t 5987\n",
      "note:47 \t 6580\n",
      "stop_note:47 \t 6580\n",
      "note:86 \t 6787\n",
      "stop_note:86 \t 6787\n",
      "tempo:110.0 \t 6917\n",
      "note:43 \t 7081\n",
      "stop_note:43 \t 7081\n",
      "note:44 \t 7304\n",
      "stop_note:44 \t 7304\n",
      "note:45 \t 7682\n",
      "stop_note:45 \t 7682\n",
      "tempo:130.0 \t 7765\n",
      "tempo:120.0 \t 7886\n",
      "note:83 \t 7897\n",
      "stop_note:83 \t 7897\n",
      "note:46 \t 8112\n",
      "stop_note:46 \t 8112\n",
      "velocity:90 \t 8486\n",
      "note:84 \t 8812\n",
      "stop_note:84 \t 8812\n",
      "note:49 \t 9103\n",
      "stop_note:49 \t 9103\n",
      "note:82 \t 9227\n",
      "stop_note:82 \t 9227\n",
      "note:80 \t 10036\n",
      "stop_note:80 \t 10036\n",
      "note:48 \t 11023\n",
      "stop_note:48 \t 11023\n",
      "note:54 \t 11284\n",
      "stop_note:54 \t 11284\n",
      "note:78 \t 11509\n",
      "stop_note:78 \t 11509\n",
      "note:50 \t 12150\n",
      "stop_note:50 \t 12150\n",
      "note:81 \t 12191\n",
      "stop_note:81 \t 12191\n",
      "note:51 \t 12375\n",
      "stop_note:51 \t 12375\n",
      "note:52 \t 12382\n",
      "stop_note:52 \t 12382\n",
      "wait:1.0 \t 13669\n",
      "note:75 \t 14871\n",
      "stop_note:75 \t 14871\n",
      "note:53 \t 15197\n",
      "stop_note:53 \t 15197\n",
      "note:73 \t 15415\n",
      "stop_note:73 \t 15415\n",
      "note:79 \t 15490\n",
      "stop_note:79 \t 15490\n",
      "note:56 \t 15810\n",
      "stop_note:56 \t 15810\n",
      "note:66 \t 16291\n",
      "stop_note:66 \t 16291\n",
      "note:77 \t 16491\n",
      "stop_note:77 \t 16491\n",
      "note:59 \t 16561\n",
      "stop_note:59 \t 16561\n",
      "note:61 \t 16571\n",
      "stop_note:61 \t 16571\n",
      "note:76 \t 16683\n",
      "stop_note:76 \t 16683\n",
      "note:68 \t 17172\n",
      "stop_note:68 \t 17172\n",
      "note:71 \t 17425\n",
      "stop_note:71 \t 17425\n",
      "note:70 \t 17640\n",
      "stop_note:70 \t 17640\n",
      "note:57 \t 18261\n",
      "stop_note:57 \t 18261\n",
      "note:58 \t 18602\n",
      "stop_note:58 \t 18602\n",
      "note:63 \t 18634\n",
      "stop_note:63 \t 18634\n",
      "note:55 \t 19088\n",
      "stop_note:55 \t 19088\n",
      "note:69 \t 20122\n",
      "stop_note:69 \t 20122\n",
      "note:64 \t 20574\n",
      "stop_note:64 \t 20574\n",
      "note:74 \t 20932\n",
      "stop_note:74 \t 20932\n",
      "note:65 \t 21076\n",
      "stop_note:65 \t 21076\n",
      "note:72 \t 21721\n",
      "stop_note:72 \t 21721\n",
      "note:67 \t 23123\n",
      "stop_note:67 \t 23123\n",
      "note:60 \t 23505\n",
      "stop_note:60 \t 23505\n",
      "note:62 \t 23615\n",
      "stop_note:62 \t 23615\n",
      "wait:0.333 \t 27655\n",
      "wait:0.167 \t 31539\n",
      "wait:0.083 \t 53312\n",
      "wait:0.5 \t 59820\n",
      "velocity:50 \t 65330\n",
      "velocity:70 \t 73229\n",
      "wait:0.25 \t 213740\n"
     ]
    }
   ],
   "source": [
    "# Gather some information of the data\n",
    "token_occurrences = Counter(training_data)\n",
    "sorted_token_occurrences = {k: v for k, v in sorted(token_occurrences.items(), key=lambda item: item[1])}\n",
    "\n",
    "print(\"Number of unique tokens: \" + str(len(sorted_token_occurrences)))\n",
    "\n",
    "print(\"Occurrences:\")\n",
    "for token, count in sorted_token_occurrences.items():\n",
    "    print(token, \"\\t\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "501ea875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', 'tempo:90.0', 'wait:0.5', 'note:81', 'note:57', 'wait:0.5', 'stop_note:81', 'stop_note:57', 'note:88', 'note:64', 'tempo:100.0', 'wait:3.0', 'stop_note:88', 'stop_note:64', 'note:86', 'velocity:50', 'note:62', 'wait:0.083', 'velocity:70', 'note:88', 'velocity:50', 'note:64', 'wait:0.167', 'stop_note:86', 'stop_note:62', 'tempo:110.0', 'velocity:70', 'note:86', 'note:62', 'wait:0.083', 'stop_note:88', 'stop_note:64', 'wait:0.417', 'stop_note:86', 'stop_note:62', 'note:84', 'note:60', 'wait:0.5', 'stop_note:84', 'stop_note:60', 'tempo:100.0', 'note:86', 'note:62', 'wait:0.5', 'stop_note:86', 'stop_note:62', 'tempo:110.0', 'note:88', 'note:64', 'wait:0.5', 'stop_note:88', 'stop_note:64', 'tempo:120.0', 'note:89', 'note:65', 'wait:0.5', 'stop_note:89', 'stop_note:65', 'note:91', 'note:67', 'wait:0.5', 'stop_note:91', 'stop_note:67', 'tempo:110.0', 'note:88', 'note:64', 'wait:0.5', 'stop_note:88', 'stop_note:64', 'note:89', 'note:65', 'wait:0.5', 'stop_note:89', 'stop_note:65', 'tempo:100.0', 'note:88', 'note:64', 'wait:0.25', 'stop_note:88', 'stop_note:64', 'note:86', 'note:88', 'note:62', 'note:64', 'wait:0.25', 'stop_note:86', 'stop_note:88', 'stop_note:62', 'stop_note:64', 'note:86', 'note:62', 'wait:1.0', 'stop_note:86', 'stop_note:62', 'tempo:90.0', 'note:84', 'note:60', 'wait:1.0', 'stop_note:84', 'stop_note:60']\n"
     ]
    }
   ],
   "source": [
    "print(training_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69eeb535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all tokens to ints\n",
    "unique_tokens = list(set(training_data))\n",
    "token_int_pairings = {token: unique_tokens.index(token) for token in unique_tokens}\n",
    "\n",
    "training_data_ints = [token_int_pairings[token] for token in training_data]\n",
    "\n",
    "# Save dictionary\n",
    "json.dump(dict(token_int_pairings.items()), open(\"./dictionary.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4c4db81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "ints_as_str = [str(token_int) for token_int in training_data_ints]\n",
    "\n",
    "with open(\"./training_data_preprocessed.txt\", \"w\") as f:\n",
    "    for idx in range(0, len(training_data_ints) - seq_length, 1):\n",
    "        f.write(' '.join(ints_as_str[idx:idx + seq_length]) + \", \" + ints_as_str[idx + seq_length] + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
