{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "545470fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "%run midi_utils.ipynb\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "549ad98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "# Dynamically grabs data for the model, since the whole thing wouldn't fit into memory all at once.\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, training_data, batch_size, num_classes, shuffle=True):\n",
    "        self.training_data = training_data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of batches\n",
    "        return int(np.floor(len(training_data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = [i.split(\", \")[0].split(\" \") for i in self.training_data[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        X = [[int(integer) for integer in integers] for integers in X]\n",
    "        y = [i.split(\", \")[1] for i in self.training_data[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        y = [int(integer) for integer in y]\n",
    "\n",
    "        return to_categorical(X, num_classes=self.num_classes), to_categorical(y, num_classes=self.num_classes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b76711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "vocabulary = {token: int(token_int) for token, token_int in json.load(open(\"./dictionary.json\")).items()}\n",
    "\n",
    "# Count the lines in training_data\n",
    "with open(\"./training_data_preprocessed.txt\") as f:\n",
    "    training_data = f.read().splitlines()\n",
    "\n",
    "# FOR TESTING THE LEARNING CAPABILITY OF THE MODEL\n",
    "training_data = training_data[:100000]\n",
    "    \n",
    "# Instantiate generator with batch size 512, shuffling the data each epoch\n",
    "training_generator = DataGenerator(training_data, 512, len(vocabulary), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3aacea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config to save model after every epoch if it is better than all previous ones in terms of minimal loss\n",
    "filepath = \"../models/deep/DeepLSTM-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='loss',\n",
    "    verbose=0,\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635ed889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to continue training an existing model, load it here\n",
    "# model = tf.keras.models.load_model(\"../models/simple/SimpleLSTM-49-0.2541.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c99773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to start training from scratch, instantiate the model here\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(100, len(vocabulary),), return_sequences = True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(256, return_sequences = True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(len(vocabulary), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e187748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "195/195 [==============================] - 48s 246ms/step - loss: 0.8363\n",
      "Epoch 2/100\n",
      "195/195 [==============================] - 49s 248ms/step - loss: 0.8344\n",
      "Epoch 3/100\n",
      "195/195 [==============================] - 48s 246ms/step - loss: 0.8332\n",
      "Epoch 4/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.8216\n",
      "Epoch 5/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.8202\n",
      "Epoch 6/100\n",
      "195/195 [==============================] - 49s 249ms/step - loss: 0.8100\n",
      "Epoch 7/100\n",
      "195/195 [==============================] - 49s 248ms/step - loss: 0.8145\n",
      "Epoch 8/100\n",
      "195/195 [==============================] - 49s 248ms/step - loss: 0.8155\n",
      "Epoch 9/100\n",
      "195/195 [==============================] - 48s 248ms/step - loss: 0.7966\n",
      "Epoch 10/100\n",
      "195/195 [==============================] - 49s 248ms/step - loss: 0.7988\n",
      "Epoch 11/100\n",
      "195/195 [==============================] - 49s 248ms/step - loss: 0.7903\n",
      "Epoch 12/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.7871\n",
      "Epoch 13/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.7809\n",
      "Epoch 14/100\n",
      "195/195 [==============================] - 49s 248ms/step - loss: 0.7802\n",
      "Epoch 15/100\n",
      "195/195 [==============================] - 48s 248ms/step - loss: 0.7778\n",
      "Epoch 16/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.7797\n",
      "Epoch 17/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.7692\n",
      "Epoch 18/100\n",
      "195/195 [==============================] - 48s 246ms/step - loss: 0.7658\n",
      "Epoch 19/100\n",
      "195/195 [==============================] - 51s 258ms/step - loss: 0.7685\n",
      "Epoch 20/100\n",
      "195/195 [==============================] - 50s 257ms/step - loss: 0.7484\n",
      "Epoch 21/100\n",
      "195/195 [==============================] - 50s 254ms/step - loss: 0.7512\n",
      "Epoch 22/100\n",
      "195/195 [==============================] - 52s 266ms/step - loss: 0.7495\n",
      "Epoch 23/100\n",
      "195/195 [==============================] - 51s 259ms/step - loss: 0.7481\n",
      "Epoch 24/100\n",
      "195/195 [==============================] - 50s 257ms/step - loss: 0.7434\n",
      "Epoch 25/100\n",
      "195/195 [==============================] - 50s 258ms/step - loss: 0.7390\n",
      "Epoch 26/100\n",
      "195/195 [==============================] - 49s 248ms/step - loss: 0.7343\n",
      "Epoch 27/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.7332\n",
      "Epoch 28/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.7270\n",
      "Epoch 29/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.7229\n",
      "Epoch 30/100\n",
      "195/195 [==============================] - 48s 248ms/step - loss: 0.7186\n",
      "Epoch 31/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.7122\n",
      "Epoch 32/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.7219\n",
      "Epoch 33/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.7120\n",
      "Epoch 34/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.7015\n",
      "Epoch 35/100\n",
      "195/195 [==============================] - 49s 249ms/step - loss: 0.7031\n",
      "Epoch 36/100\n",
      "195/195 [==============================] - 48s 247ms/step - loss: 0.7111\n",
      "Epoch 37/100\n",
      "195/195 [==============================] - 48s 248ms/step - loss: 0.6979\n",
      "Epoch 38/100\n",
      "195/195 [==============================] - 51s 260ms/step - loss: 0.6888\n",
      "Epoch 39/100\n",
      "195/195 [==============================] - 51s 261ms/step - loss: 0.6891\n",
      "Epoch 40/100\n",
      "195/195 [==============================] - 50s 257ms/step - loss: 0.6819\n",
      "Epoch 41/100\n",
      "195/195 [==============================] - 50s 253ms/step - loss: 0.6842\n",
      "Epoch 42/100\n",
      "195/195 [==============================] - 50s 255ms/step - loss: 0.6789\n",
      "Epoch 43/100\n",
      "195/195 [==============================] - 50s 253ms/step - loss: 0.6781\n",
      "Epoch 44/100\n",
      "195/195 [==============================] - 50s 253ms/step - loss: 0.6783\n",
      "Epoch 45/100\n",
      "195/195 [==============================] - 50s 254ms/step - loss: 0.6739\n",
      "Epoch 46/100\n",
      "195/195 [==============================] - 53s 273ms/step - loss: 0.6663\n",
      "Epoch 47/100\n",
      "195/195 [==============================] - 52s 267ms/step - loss: 0.6681\n",
      "Epoch 48/100\n",
      "195/195 [==============================] - 50s 254ms/step - loss: 0.6655\n",
      "Epoch 49/100\n",
      "195/195 [==============================] - 50s 256ms/step - loss: 0.6557\n",
      "Epoch 50/100\n",
      "195/195 [==============================] - 50s 254ms/step - loss: 0.6556\n",
      "Epoch 51/100\n",
      "195/195 [==============================] - 51s 259ms/step - loss: 0.6513\n",
      "Epoch 52/100\n",
      "195/195 [==============================] - 53s 271ms/step - loss: 0.6560\n",
      "Epoch 53/100\n",
      "195/195 [==============================] - 51s 259ms/step - loss: 0.6521\n",
      "Epoch 54/100\n",
      "195/195 [==============================] - 50s 255ms/step - loss: 0.6453\n",
      "Epoch 55/100\n",
      "195/195 [==============================] - 50s 256ms/step - loss: 0.6512\n",
      "Epoch 56/100\n",
      "195/195 [==============================] - 51s 261ms/step - loss: 0.6442\n",
      "Epoch 57/100\n",
      "195/195 [==============================] - 52s 265ms/step - loss: 0.6390\n",
      "Epoch 58/100\n",
      "195/195 [==============================] - 50s 253ms/step - loss: 0.6341\n",
      "Epoch 59/100\n",
      "195/195 [==============================] - 50s 256ms/step - loss: 0.6302\n",
      "Epoch 60/100\n",
      "195/195 [==============================] - 50s 255ms/step - loss: 0.6304\n",
      "Epoch 61/100\n",
      "195/195 [==============================] - 50s 253ms/step - loss: 0.6337\n",
      "Epoch 62/100\n",
      "195/195 [==============================] - 49s 253ms/step - loss: 0.6299\n",
      "Epoch 63/100\n",
      "195/195 [==============================] - 50s 255ms/step - loss: 0.6218\n",
      "Epoch 64/100\n",
      "195/195 [==============================] - 50s 254ms/step - loss: 0.6228\n",
      "Epoch 65/100\n",
      "195/195 [==============================] - 53s 272ms/step - loss: 0.6212\n",
      "Epoch 66/100\n",
      "195/195 [==============================] - 52s 268ms/step - loss: 0.6163\n",
      "Epoch 67/100\n",
      "195/195 [==============================] - 52s 268ms/step - loss: 0.6174\n",
      "Epoch 68/100\n",
      "195/195 [==============================] - 52s 268ms/step - loss: 0.6137\n",
      "Epoch 69/100\n",
      "195/195 [==============================] - 52s 267ms/step - loss: 0.6143\n",
      "Epoch 70/100\n",
      "195/195 [==============================] - 52s 268ms/step - loss: 0.6062\n",
      "Epoch 71/100\n",
      "195/195 [==============================] - 53s 268ms/step - loss: 0.6157\n",
      "Epoch 72/100\n",
      "195/195 [==============================] - 53s 268ms/step - loss: 0.6036\n",
      "Epoch 73/100\n",
      "195/195 [==============================] - 53s 272ms/step - loss: 0.6025\n",
      "Epoch 74/100\n",
      "195/195 [==============================] - 53s 269ms/step - loss: 0.6019\n",
      "Epoch 75/100\n",
      "195/195 [==============================] - 53s 269ms/step - loss: 0.5947\n",
      "Epoch 76/100\n",
      "195/195 [==============================] - 56s 286ms/step - loss: 0.5921\n",
      "Epoch 77/100\n",
      "195/195 [==============================] - 52s 264ms/step - loss: 0.5966\n",
      "Epoch 78/100\n",
      "195/195 [==============================] - 51s 260ms/step - loss: 0.5937\n",
      "Epoch 79/100\n",
      "195/195 [==============================] - 53s 272ms/step - loss: 0.5885\n",
      "Epoch 80/100\n",
      "195/195 [==============================] - 52s 266ms/step - loss: 0.5868\n",
      "Epoch 81/100\n",
      "195/195 [==============================] - 51s 262ms/step - loss: 0.5759\n",
      "Epoch 82/100\n",
      "195/195 [==============================] - 51s 259ms/step - loss: 0.5827\n",
      "Epoch 83/100\n",
      "195/195 [==============================] - 51s 259ms/step - loss: 0.5797\n",
      "Epoch 84/100\n",
      "195/195 [==============================] - 53s 269ms/step - loss: 0.5701\n",
      "Epoch 85/100\n",
      "195/195 [==============================] - 51s 259ms/step - loss: 0.5775\n",
      "Epoch 86/100\n",
      "195/195 [==============================] - 50s 257ms/step - loss: 0.5810\n",
      "Epoch 87/100\n",
      "195/195 [==============================] - 52s 264ms/step - loss: 0.5715\n",
      "Epoch 88/100\n",
      "195/195 [==============================] - 51s 262ms/step - loss: 0.5663\n",
      "Epoch 89/100\n",
      "195/195 [==============================] - 51s 259ms/step - loss: 0.5684\n",
      "Epoch 90/100\n",
      "195/195 [==============================] - 51s 259ms/step - loss: 0.5684\n",
      "Epoch 91/100\n",
      "195/195 [==============================] - 51s 261ms/step - loss: 0.5648\n",
      "Epoch 92/100\n",
      "195/195 [==============================] - 51s 260ms/step - loss: 0.5627\n",
      "Epoch 93/100\n",
      "195/195 [==============================] - 51s 259ms/step - loss: 0.5580\n",
      "Epoch 94/100\n",
      "195/195 [==============================] - 50s 257ms/step - loss: 0.5632\n",
      "Epoch 95/100\n",
      "195/195 [==============================] - 51s 262ms/step - loss: 0.5543\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34/195 [====>.........................] - ETA: 41s - loss: 0.5394"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-827b37c9de34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[1;31m#use_multiprocessing=True, can't use this in a jupyter notebook ¯\\_(ツ)_/¯\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m           workers=6)\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\envs\\maprojekt\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\maprojekt\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\maprojekt\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\maprojekt\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\maprojekt\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\maprojekt\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\maprojekt\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(training_generator,\n",
    "          epochs=100,\n",
    "          #use_multiprocessing=True, can't use this in a jupyter notebook ¯\\_(ツ)_/¯ \n",
    "          callbacks=[checkpoint],\n",
    "          workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fce65e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loading(i, n_tokens_to_generate, stop_at_EOS):\n",
    "    clear_output(wait=True)\n",
    "    if not stop_at_EOS:\n",
    "        print(str(i), \"/\", str(n_tokens_to_generate), \"generated\")\n",
    "        return\n",
    "    print(str(i), \"/ ?\", \"generated\")\n",
    "\n",
    "def generate_music(model, vocab_size, vocabulary, starting_input, n_tokens_to_generate, stop_at_EOS = False):\n",
    "    \n",
    "    # Keeps track of the number of tokens generated so far\n",
    "    i = 0\n",
    "    \n",
    "    # Used as input, where the first input is a bunch of random tokens from the vocabulary \n",
    "    # It's sliding because the predicted token will be constantly appended to the input\n",
    "    # [0, 1, 2] -predict-> [3] \n",
    "    # [1, 2, 3] -predict-> [4]\n",
    "    # [2, 3, 4] and so on\n",
    "    if starting_input == None:\n",
    "        sliding_window = [np.random.randint(0, vocab_size, size=100).tolist()]\n",
    "    else:\n",
    "        sliding_window = [starting_input]\n",
    "    \n",
    "    # Inverse of the vocabulary, because the tokens in integer form need to be converted back to tokens\n",
    "    int_to_token_dict = dict(map(reversed, vocabulary.items()))\n",
    "    \n",
    "    # List that holds the final output. Grows by each prediction.\n",
    "    prediction_output = []\n",
    "    \n",
    "    while True:\n",
    "        # Convert to the same format as the one the model saw during training\n",
    "        prediction_input = to_categorical(sliding_window, num_classes = vocab_size)\n",
    "\n",
    "        # Predict next token depending on the current sequence \n",
    "        prediction = model(prediction_input)\n",
    "        i += 1\n",
    "        \n",
    "        # Get the integer variant of the token\n",
    "        index = np.argmax(prediction)\n",
    "\n",
    "        # Check if previous tokens were \"varied\" enough: if they had at least 15 unique tokens.\n",
    "        # If not, choose a random prediction from the top 2 predictions. This avoids getting stuck in a short melody.\n",
    "        if (len(np.unique(sliding_window)) < 40):\n",
    "            indexes_of_top2_predictions = np.argpartition(prediction[0], -2)[-2:]\n",
    "            index = np.random.choice(indexes_of_top2_predictions)\n",
    "        \n",
    "        # Grab the token variant of the integer and append the resulting token to prediction output\n",
    "        result = int_to_token_dict[index]\n",
    "        prediction_output.append(result)\n",
    "        \n",
    "        # Slide the input 1 int to the right, appending the current prediction and removing one token from the start,\n",
    "        # so the sequence length will stay the same\n",
    "        sliding_window = np.append(sliding_window, index)\n",
    "        sliding_window = [sliding_window[1:len(sliding_window)]]\n",
    "        \n",
    "        # A loading bar for the impatient\n",
    "        print_loading(i, n_tokens_to_generate, stop_at_EOS)\n",
    "        \n",
    "        if (stop_at_EOS and result == \"<EOS>\") or (i == n_tokens_to_generate):\n",
    "            break\n",
    "            \n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0819b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 / 3000 generated\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv67606'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv67606');\n",
       "                               mp.base64Load('data:audio/midi;base64,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');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_tokens = generate_music(model, len(vocabulary), vocabulary, training_data[0].split(\", \")[0].split(\" \"), 3000)\n",
    "convert_tokens_to_midi(generated_tokens).show(\"midi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c637d6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " ['note:82',\n",
       "  'note:82',\n",
       "  'note:83',\n",
       "  'note:83',\n",
       "  'note:82',\n",
       "  'note:37',\n",
       "  'note:87',\n",
       "  'note:36'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_tokens(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea1e8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 / 3000 generated\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 2):\n",
    "    midi_filepath = \"../generated_samples/deep_sample\" + str(i) + \".mid\"\n",
    "    \n",
    "    generated_tokens = generate_music(model, len(vocabulary), vocabulary, 3000)\n",
    "    generated_midi_stream = convert_tokens_to_midi(generated_tokens)\n",
    "    generated_midi_stream.write('midi', fp=midi_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
