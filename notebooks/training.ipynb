{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545470fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import json\n",
    "%run midi_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ad98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "vocabulary = {token: int(token_int) for token, token_int in json.load(open(\"./dictionary.json\")).items()}\n",
    "vocabulary\n",
    "\n",
    "# Count the lines in training_data\n",
    "with open(\"./training_data_preprocessed.txt\") as f:\n",
    "    training_data = f.read().splitlines() \n",
    "number_of_lines = len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b76711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, training_data, batch_size, num_classes, shuffle = True):\n",
    "        self.training_data = training_data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle  \n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of batches\n",
    "        return int(np.floor(len(training_data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = [i.split(\", \")[0].split(\" \") for i in self.training_data[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        X = [[int(integer) for integer in integers] for integers in X]\n",
    "        y = [i.split(\", \")[1] for i in self.training_data[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        y = [int(integer) for integer in y]\n",
    "    \n",
    "        return to_categorical(X, num_classes=self.num_classes), to_categorical(y, num_classes=self.num_classes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.training_data)\n",
    "            \n",
    "training_generator = CustomDataset(training_data, 4, len(vocabulary), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a415bf1e",
   "metadata": {},
   "source": [
    "## NB! For whatever reason, Jupyter Notebook doesn't like the following. This has been moved to training.py, where it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e187748",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(100, len(vocabulary),)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(vocabulary), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(training_generator,\n",
    "          use_multiprocessing=True,\n",
    "          workers=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
